{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce932353-1a95-4b24-a9d4-04c807e59e31",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The Content Creation Assistant notebook harnesses cutting-edge artificial intelligence to streamline and enhance digital content creation. It integrates advanced tools and frameworks to offer a comprehensive solution for both written and visual content generation.\n",
    "\n",
    "At its core is the LlamaIndex framework, which efficiently manages complex workflows and integrates various AI models and APIs. The Tavily Search API provides real-time, relevant information to ensure that the content is engaging, informative, and timely.\n",
    "\n",
    "For text generation, the notebook utilizes the OpenAI GPT model, known for producing coherent and contextually accurate text, ideal for blog posts and social media updates. Complementing this, the OpenAI DALL-E 3 model generates unique images based on the themes of the text, enhancing visual appeal and reader engagement.\n",
    "\n",
    "Overall, the Content Creation Assistant empowers users—whether bloggers, marketers, or social media managers—to produce rich, multi-dimensional content effortlessly, saving time and boosting creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d160ca71-50fc-48f7-8ac9-ac70360645ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-core in /opt/conda/lib/python3.12/site-packages (0.11.23)\n",
      "Requirement already satisfied: llama-index-utils-workflow in /opt/conda/lib/python3.12/site-packages (0.2.2)\n",
      "Requirement already satisfied: asyncio in /opt/conda/lib/python3.12/site-packages (3.4.3)\n",
      "Requirement already satisfied: llama-index-llms-openai in /opt/conda/lib/python3.12/site-packages (0.2.16)\n",
      "Requirement already satisfied: tavily-python in /opt/conda/lib/python3.12/site-packages (0.5.0)\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.12/site-packages (1.54.4)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (2024.10.0)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (11.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.12/site-packages (from llama-index-core) (1.16.0)\n",
      "Requirement already satisfied: pyvis<0.4.0,>=0.3.2 in /opt/conda/lib/python3.12/site-packages (from llama-index-utils-workflow) (0.3.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx->llama-index-core) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx->llama-index-core) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core) (2.23.4)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (8.29.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /opt/conda/lib/python3.12/site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.1.4)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /opt/conda/lib/python3.12/site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (4.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.12/site-packages (from dataclasses-json->llama-index-core) (3.23.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2>=2.9.6->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.12/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.12/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-core llama-index-utils-workflow asyncio llama-index-llms-openai tavily-python openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171dc7a-4c0c-4f45-81d8-90be33e4fd9d",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7907cd65-ce72-4e4c-907d-2ff2384c953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    "    Event\n",
    ")\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from tavily import TavilyClient\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated\n",
    "from openai import OpenAI as dalle3 # use specifically for image generation\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import uuid\n",
    "import os\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72434945-8409-4bde-b413-5465b50a1727",
   "metadata": {},
   "source": [
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e89296d2-4533-46e4-8bd1-afe8598b36a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n",
      " ········\n"
     ]
    }
   ],
   "source": [
    "tavily_api_key = getpass.getpass()\n",
    "openai_api_key = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb4c8b-c3ec-487c-8c52-4fd6b18cf434",
   "metadata": {},
   "source": [
    "## Setting up Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d210a18-7389-4332-9252-aba1295078fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[\\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.youtube.com/user/KendrickLamarVEVO\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Kendrick Lamar on Vevo - Official Music Videos, Live Performances, Interviews and more\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.youtube.com/channel/UC3lBXcrKFnFAFkfVk5WuKcQ\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Watch Kendrick Lamar's official music videos, live performances, interviews and more on his YouTube channel.\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://variety.com/2024/music/reviews/kendrick-lamar-gnx-masterpiece-album-review-1236222041/\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Midway through his snarling Drake diss, \\\\\\\\u201cNot Like Us,\\\\\\\\u201d Kendrick Lamar issued a succinct, but forceful personal mission: \\\\\\\\u201cSometimes you gotta pop out and show n\\\\\\\\u2014as.\\\\\\\\u201d It was both a plan of action and a self-fulfilling mandate. Since then, he\\\\\\\\u2019s won that rap beef in the most unequivocal terms imaginable: \\\\\\\\u201cNot Like Us\\\\\\\\u201d has been nominated for multiple Grammys, a rare diss track to achieve that status (ironically the last was Drake\\\\\\\\u2019s Meek Mill swipe \\\\\\\\u201cBack to Back\\\\\\\\u201d), the peak of a six-song flurry, beginning with his guest verse of Future and Metro Boomin\\\\\\\\u2019s \\\\\\\\u201cLike That,\\\\\\\\u201d that got just one response before its target conceded with silence.\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.britannica.com/biography/Kendrick-Lamar\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"The following year Lamar received 11 Grammy nominations, and he won for best rap performance and best rap song (both for \\\\\\\\u201cAlright\\\\\\\\u201d), best rap/sung collaboration (for \\\\\\\\u201cThese Walls\\\\\\\\u201d), best music video (for \\\\\\\\u201cBad Blood\\\\\\\\u201d), and best rap album (for To Pimp a Butterfly). He won Grammys for best rap album (DAMN.), best rap song, rap performance, and music video (all for \\\\\\\\u201cHUMBLE.\\\\\\\\u201d), and best rap/sung performance (\\\\\\\\u201cLOYALTY.\\\\\\\\u201d; with Rihanna). In 2011 Top Dawg made Lamar\\\\\\\\u2019s album Section.80 available on iTunes, and at a concert that same year Lamar was ceremonially declared the \\\\\\\\u201cnew king of the West Coast\\\\\\\\u201d by veteran rap artists Game, Snoop Dogg, and Dr. Dre.\\\\\\\\n Lamar produced and curated the album, a collection of songs \\\\\\\\u201cfrom and inspired by\\\\\\\\u201d the movie, and he performed on every track. It won Grammys for best rap album, rap song, and rap performance; the latter two awards were for \\\\\\\\u201cThe Heart Part 5.\\\\\\\\u201d\\\\\\\"}\\\", \\\"{\\\\\\\"url\\\\\\\": \\\\\\\"https://www.biography.com/musicians/kendrick-lamar\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"(Profile photo of Kendrick Lamar by Jason Merritt/Getty Images)\\\\\\\\nQUICK FACTS\\\\\\\\nQuotes\\\\\\\\nGrammy Awards\\\\\\\\nBillie Eilish\\\\\\\\nJohn Williams\\\\\\\\nAdele\\\\\\\\nJelly Roll\\\\\\\\nC\\\\\\\\u00e9line Dion\\\\\\\\nToby Keith\\\\\\\\nJoni Mitchell\\\\\\\\n2024 Grammys: The Major Winners and Takeaways\\\\\\\\nTaylor Swift\\\\\\\\nSZA\\\\\\\\nMiley Cyrus\\\\\\\\nZach Bryan\\\\\\\\nA Part of Hearst Digital Media\\\\\\\\n (Lady Gaga recorded a song with Lamar for\\\\\\\\nthe album, but it ultimately was not included due to \\\\\\\\\\\\\\\"creative\\\\\\\\ndifferences.\\\\\\\\\\\\\\\") Hit singles like \\\\\\\\\\\\\\\"Swimming Pools (Drank)\\\\\\\\\\\\\\\" and \\\\\\\\\\\\\\\"Poetic\\\\\\\\nJustice,\\\\\\\\\\\\\\\" and the rapper's emergence as a talent to watch, cleared the\\\\\\\\nway for him to make major American television appearances while\\\\\\\\npromoting the album, including Saturday Night Live, Late Night With David Letterman and Late Night With Jimmy Fallon.\\\\\\\\n The\\\\\\\\nthought-provoking lyrics on his debut album caught the attention of\\\\\\\\nhip-hop critics as well, with MTV naming him the \\\\\\\\\\\\\\\"Hottest MC\\\\\\\\\\\\\\\" of\\\\\\\\n2012\\\\\\\\u2014putting him in the company of other rappers who have earned the\\\\\\\\ntitle, including Lil Wayne, Jay-Z and Kanye West.\\\\\\\\n Pulitzer Prize Winner\\\\\\\\nLamar made history in April 2018 when he won a Pulitzer Prize for music for DAMN., making him not only the first person to win a Pulitzer for a hip-hop album, but also the first artist to win the prize for music that isn't classical or jazz. He later brought the house down with a politicized performance of \\\\\\\\\\\\\\\"The Blacker the Berry\\\\\\\\\\\\\\\" and \\\\\\\\\\\\\\\"Alright\\\\\\\\\\\\\\\" that fused spoken word, live jazz, traditional African dance and a reference to the death of teen Trayvon Martin.\\\\\\\\n\\\\\\\"}\\\"]\"\n"
     ]
    }
   ],
   "source": [
    "tavily_client = TavilyClient(api_key=tavily_api_key)\n",
    "response = tavily_client.get_search_context(\"Kendrick Lamar\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4adb3b53-8cee-4dab-86e3-86f22ccdaf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TavilySearchInput(BaseModel):\n",
    "    query: Annotated[str, Field(description=\"The search query string\")]\n",
    "    max_results: Annotated[\n",
    "        int, Field(description=\"Maximum number of results to return\", ge=1, le=10)\n",
    "    ] = 5\n",
    "    search_depth: Annotated[\n",
    "        str,\n",
    "        Field(\n",
    "            description=\"Search depth: 'basic' or 'advanced'\",\n",
    "            choices=[\"basic\", \"advanced\"],\n",
    "        ),\n",
    "    ] = \"basic\"\n",
    "\n",
    "def tavily_search(query: Annotated[TavilySearchInput, \"Input for Tavily search\"]):\n",
    "    tavily_client = TavilyClient(api_key=tavily_api_key)\n",
    "    # Perform the search\n",
    "    response = tavily_client.search(\n",
    "        query=query.query,\n",
    "        max_results=query.max_results,\n",
    "        search_depth=query.search_depth,\n",
    "    )\n",
    "\n",
    "    # Format the results\n",
    "    formatted_results = []\n",
    "    for result in response.get(\"results\", []):\n",
    "        formatted_results.append(\n",
    "            f\"Title: {result['title']}\\\\nURL: {result['url']}\\\\nContent: {result['content']}\\\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\\\n\".join(formatted_results)\n",
    "\n",
    "def save_file(content, uuid, type=\"Blog\"):\n",
    "        # Define the filename for the markdown file\n",
    "        directory = uuid\n",
    "        file_name = f\"{type}_{uuid}.md\"\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "        try:\n",
    "            # Create the directory if it doesn't exist\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "            # Open the file in write mode ('w') and save the blog content\n",
    "            with open(file_path, 'w') as file:\n",
    "                file.write(f\"# {content}\")\n",
    "            print(f\"{type} post saved to {file_path}.\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred while saving the blog post:\", e)\n",
    "\n",
    "async def generate_image_with_retries(client, prompt, max_retries=3, delay=2):\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            # Attempt to generate the image using the dalle3 API\n",
    "            response = client.images.generate(\n",
    "                model=\"dall-e-3\",\n",
    "                prompt=prompt,\n",
    "                size=\"1024x1024\",\n",
    "                quality=\"standard\",\n",
    "                n=1,\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            print(f\"Attempt {attempt} failed with error: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                await asyncio.sleep(delay)\n",
    "                print(f\"Retrying after {delay} seconds...\")\n",
    "            else:\n",
    "                print(\"Exceeded maximum retries.\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd46f0-f977-40e8-ba24-ab54bc98e323",
   "metadata": {},
   "source": [
    "## Setup Prompt Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f11ba70-2e45-453f-8af9-79ea344938db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts.prompts import *\n",
    "\n",
    "blog_template = BLOG_TEMPLATE\n",
    "blog_and_research_template = BLOG_AND_RESEARCH_TEMPLATE\n",
    "image_prompt_instructions = IMAGE_GENERATION_TEMPLATE\n",
    "linked_in_template = LINKED_IN_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56995989-5f64-4e50-b7d7-300101dcc3fd",
   "metadata": {},
   "source": [
    "## Setting up Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791d7041-d735-4466-af5d-c17cb80ebb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchEvent(Event):\n",
    "    query: str\n",
    "    uuid: str\n",
    "\n",
    "class BlogEvent(Event):\n",
    "    query: str\n",
    "    research: str\n",
    "    uuid: str\n",
    "\n",
    "class BlogWithoutResearch(Event):\n",
    "    query: str\n",
    "    uuid: str\n",
    "\n",
    "class SocialMediaEvent(Event):\n",
    "    blog: str\n",
    "    uuid: str\n",
    "\n",
    "class SocialMediaCompleteEvent(Event):\n",
    "    result: str\n",
    "\n",
    "class IllustratorEvent(Event):\n",
    "    blog: str\n",
    "    \n",
    "class IllustratorCompleteEvent(Event):\n",
    "    result: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86aa5b3f-1825-4405-bfe4-3281f6d2d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentCreationWorkflow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def start(self, ctx: Context, ev: StartEvent) -> ResearchEvent | BlogWithoutResearch :\n",
    "        print(\"Starting content creation\", ev.query)\n",
    "        id = str(uuid.uuid4())\n",
    "        if (ev.research) is False:\n",
    "            return BlogWithoutResearch(query=ev.query, uuid=id)\n",
    "        return ResearchEvent(query=ev.query, uuid=id)\n",
    "\n",
    "    @step\n",
    "    async def step_research(self, ctx: Context, ev: ResearchEvent) -> BlogEvent:\n",
    "        print(\"Researching users query\")\n",
    "        search_input = TavilySearchInput(\n",
    "            query=ev.query,\n",
    "            max_results=3,\n",
    "            search_depth=\"basic\")\n",
    "        research = tavily_search(search_input)\n",
    "        return BlogEvent(query=ev.query, research=research, uuid=ev.uuid)\n",
    "\n",
    "    @step\n",
    "    async def step_blog_without_research(self, ctx: Context, ev: BlogWithoutResearch) -> SocialMediaEvent | IllustratorEvent:\n",
    "        print(\"Writing blog post without research\")\n",
    "        print(\"uuid\", ev.uuid)\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key)\n",
    "        prompt = blog_template.format(query_str=ev.query)\n",
    "        result = await llm.acomplete(prompt, formatted=True)\n",
    "        save_file(result.text, ev.uuid)\n",
    "        print(result)\n",
    "        ctx.send_event(SocialMediaEvent(blog=result.text, uuid=ev.uuid))\n",
    "        ctx.send_event(IllustratorEvent(blog=result.text, uuid=ev.uuid))\n",
    "                        \n",
    "    @step\n",
    "    async def step_blog(self, ctx: Context, ev: BlogEvent) -> SocialMediaEvent | IllustratorEvent:\n",
    "        print(\"Writing blog post\")\n",
    "\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key)\n",
    "        prompt = blog_and_research_template.format(query_str=ev.query, research=ev.research)\n",
    "        result = await llm.acomplete(prompt, formatted=True)\n",
    "\n",
    "        save_file(result.text, ev.uuid)\n",
    "        ctx.send_event(SocialMediaEvent(blog=result.text, uuid=ev.uuid))\n",
    "        ctx.send_event(IllustratorEvent(blog=result.text, uuid=ev.uuid))\n",
    "\n",
    "    @step\n",
    "    async def step_social_media(self, ctx: Context, ev: SocialMediaEvent) -> SocialMediaCompleteEvent:\n",
    "        print(\"Writing social media post\")\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key)\n",
    "        prompt = linked_in_template.format(blog_content=ev.blog)\n",
    "        results = await llm.acomplete(prompt, formatted=True)\n",
    "        save_file(results.text, ev.uuid, type=\"LinkedIn\")\n",
    "        return SocialMediaCompleteEvent(result=\"LinkedIn post written\")\n",
    "\n",
    "    @step\n",
    "    async def step_illustrator(self, ctx: Context, ev:IllustratorEvent) -> IllustratorCompleteEvent:\n",
    "        print(\"Generating image\")\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key)\n",
    "        image_prompt_instruction_generator = image_prompt_instructions.format(blog_post=ev.blog)\n",
    "        image_prompt = await llm.acomplete(image_prompt_instruction_generator, formatted=True)\n",
    "        \n",
    "        client = dalle3(api_key=openai_api_key)\n",
    "        response = await generate_image_with_retries(client, image_prompt.text)\n",
    "        image_url = response.data[0].url\n",
    "        response = requests.get(image_url)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        \n",
    "        directory = f'./{ev.uuid}'\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        image.save(f'{directory}/generated_image.png')\n",
    "        image.save(f'{ev.uuid}/generated_image.png')\n",
    "        \n",
    "        return IllustratorCompleteEvent(result=\"Images drawn\")\n",
    "\n",
    "    @step\n",
    "    async def step_collection(self, ctx: Context, ev: SocialMediaCompleteEvent | IllustratorCompleteEvent) -> StopEvent:\n",
    "        if (\n",
    "            ctx.collect_events(\n",
    "                ev,\n",
    "                [SocialMediaCompleteEvent, IllustratorCompleteEvent]\n",
    "            ) is None\n",
    "        ) : return None\n",
    "        return StopEvent(result=\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7167ef3b-6b4e-4b59-9092-25d453724992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting content creation Kendrick Lamar\n",
      "Researching users query\n",
      "Writing blog post\n",
      "Blog post saved to 38e418b1-bcba-4d85-88dc-6d57a65def09/Blog_38e418b1-bcba-4d85-88dc-6d57a65def09.md.\n",
      "Generating image\n",
      "Writing social media post\n",
      "LinkedIn post saved to 38e418b1-bcba-4d85-88dc-6d57a65def09/LinkedIn_38e418b1-bcba-4d85-88dc-6d57a65def09.md.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "w = ContentCreationWorkflow(timeout=120, verbose=False)\n",
    "result = await w.run(query=\"Kendrick Lamar\", research=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5522ced7-6714-4990-910c-e56858362a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04cad0c3-32c2-45c6-8866-5de0f64647be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class '__main__.ResearchEvent'>\n",
      "<class '__main__.BlogWithoutResearch'>\n",
      "<class '__main__.SocialMediaEvent'>\n",
      "<class '__main__.IllustratorEvent'>\n",
      "<class '__main__.SocialMediaEvent'>\n",
      "<class '__main__.IllustratorEvent'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "<class '__main__.IllustratorCompleteEvent'>\n",
      "<class '__main__.BlogEvent'>\n",
      "<class '__main__.SocialMediaCompleteEvent'>\n",
      "content_creation_workflow.html\n"
     ]
    }
   ],
   "source": [
    "draw_all_possible_flows(ContentCreationWorkflow, \"content_creation_workflow.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e232c726-9cd4-4b17-b4ca-d9a6a28c1e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
